name: Code Linting and Quality Checks

on:
  pull_request:
    branches:
      - main
      - develop
  push:
    branches:
      - main
      - develop

jobs:
  lint:
    name: Java Code Linting
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up JDK 1.8
        uses: actions/setup-java@v4
        with:
          java-version: '8.0.442+6'
          distribution: 'temurin'
          cache: 'maven'

      - name: Cache Maven packages
        uses: actions/cache@v4
        with:
          path: ~/.m2
          key: ${{ runner.os }}-m2-${{ hashFiles('**/pom.xml') }}
          restore-keys: ${{ runner.os }}-m2

      - name: Compile project
        id: compile
        run: |
          mvn clean compile -DskipTests 2>&1 | tee compile.log
          echo "compile_status=${PIPESTATUS[0]}" >> $GITHUB_OUTPUT
        continue-on-error: true

      - name: Run Checkstyle (Report Only)
        run: mvn checkstyle:checkstyle || true
        continue-on-error: true

      - name: Run PMD (Report Only)
        run: mvn pmd:pmd || true
        continue-on-error: true

      - name: Run SpotBugs (Report Only)
        run: mvn spotbugs:spotbugs || true
        continue-on-error: true

      - name: Run TestNG Tests
        run: |
          echo "Attempting to run tests..."
          if [ -f "testng.xml" ]; then
            echo "Found testng.xml, attempting to run with suite file..."
            mvn test -DsuiteXmlFile=testng.xml || {
              echo "testng.xml failed, trying without suite file..."
              mvn test || echo "Tests completed with issues"
            }
          else
            echo "No testng.xml found, running all tests..."
            mvn test || echo "Tests completed with issues"
          fi
        continue-on-error: true

      - name: Check for test results
        id: test_check
        run: |
          if [ -d "target/surefire-reports" ] && [ "$(ls -A target/surefire-reports/*.xml 2>/dev/null)" ]; then
            echo "tests_exist=true" >> $GITHUB_OUTPUT
            echo "Test results found"
          else
            echo "tests_exist=false" >> $GITHUB_OUTPUT
            echo "No test results found"
          fi

      - name: Publish Test Results
        if: steps.test_check.outputs.tests_exist == 'true'
        uses: EnricoMi/publish-unit-test-result-action@v2
        with:
          files: |
            target/surefire-reports/TEST-*.xml
          check_name: TestNG Results
          comment_mode: off

      - name: Generate Quality Report Summary
        if: always()
        run: |
          echo "## üìä Code Quality Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Build Date:** $(date)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Compilation Status
          echo "### üî® Compilation" >> $GITHUB_STEP_SUMMARY
          if [ "${{ steps.compile.outputs.compile_status }}" == "0" ]; then
            echo "‚úÖ **Status:** SUCCESS" >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå **Status:** FAILED" >> $GITHUB_STEP_SUMMARY
            if [ -f "compile.log" ]; then
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "<details><summary>Click to see compilation errors</summary>" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
              grep -A 5 "\[ERROR\]" compile.log | head -50 >> $GITHUB_STEP_SUMMARY || echo "Check workflow logs for details"
              echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
              echo "</details>" >> $GITHUB_STEP_SUMMARY
            fi
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Checkstyle Summary
          if [ -f "target/checkstyle-result.xml" ]; then
            CHECKSTYLE_ERRORS=$(grep -o 'severity="error"' target/checkstyle-result.xml | wc -l || echo "0")
            CHECKSTYLE_WARNINGS=$(grep -o 'severity="warning"' target/checkstyle-result.xml | wc -l || echo "0")
            CHECKSTYLE_INFO=$(grep -o 'severity="info"' target/checkstyle-result.xml | wc -l || echo "0")
            echo "### ‚úÖ Checkstyle Analysis" >> $GITHUB_STEP_SUMMARY
            echo "| Severity | Count |" >> $GITHUB_STEP_SUMMARY
            echo "|----------|-------|" >> $GITHUB_STEP_SUMMARY
            echo "| Errors | $CHECKSTYLE_ERRORS |" >> $GITHUB_STEP_SUMMARY
            echo "| Warnings | $CHECKSTYLE_WARNINGS |" >> $GITHUB_STEP_SUMMARY
            echo "| Info | $CHECKSTYLE_INFO |" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          else
            echo "### ‚ö†Ô∏è Checkstyle - No results" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi
          
          # PMD Summary
          if [ -f "target/pmd.xml" ]; then
            PMD_VIOLATIONS=$(grep -c '<violation' target/pmd.xml || echo "0")
            echo "### ‚úÖ PMD Analysis" >> $GITHUB_STEP_SUMMARY
            echo "- Total Violations: **$PMD_VIOLATIONS**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          else
            echo "### ‚ö†Ô∏è PMD - No results" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi
          
          # SpotBugs Summary
          if [ -f "target/spotbugsXml.xml" ]; then
            SPOTBUGS_BUGS=$(grep -c '<BugInstance' target/spotbugsXml.xml || echo "0")
            echo "### ‚úÖ SpotBugs Analysis" >> $GITHUB_STEP_SUMMARY
            echo "- Potential Bugs: **$SPOTBUGS_BUGS**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          else
            echo "### ‚ö†Ô∏è SpotBugs - No results" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Test Summary
          if [ -d "target/surefire-reports" ] && [ "$(ls -A target/surefire-reports/TEST-*.xml 2>/dev/null)" ]; then
            echo "### ‚úÖ Test Execution" >> $GITHUB_STEP_SUMMARY
            TOTAL=$(find target/surefire-reports -name "TEST-*.xml" -exec grep -h "tests=" {} \; | sed 's/.*tests="\([0-9]*\)".*/\1/' | awk '{s+=$1} END {print s}')
            FAILURES=$(find target/surefire-reports -name "TEST-*.xml" -exec grep -h "failures=" {} \; | sed 's/.*failures="\([0-9]*\)".*/\1/' | awk '{s+=$1} END {print s}')
            ERRORS=$(find target/surefire-reports -name "TEST-*.xml" -exec grep -h "errors=" {} \; | sed 's/.*errors="\([0-9]*\)".*/\1/' | awk '{s+=$1} END {print s}')
            SKIPPED=$(find target/surefire-reports -name "TEST-*.xml" -exec grep -h "skipped=" {} \; | sed 's/.*skipped="\([0-9]*\)".*/\1/' | awk '{s+=$1} END {print s}')
            PASSED=$((${TOTAL:-0} - ${FAILURES:-0} - ${ERRORS:-0} - ${SKIPPED:-0}))
            
            echo "| Status | Count |" >> $GITHUB_STEP_SUMMARY
            echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
            echo "| ‚úÖ Passed | $PASSED |" >> $GITHUB_STEP_SUMMARY
            echo "| ‚ùå Failed | ${FAILURES:-0} |" >> $GITHUB_STEP_SUMMARY
            echo "| ‚ö†Ô∏è Errors | ${ERRORS:-0} |" >> $GITHUB_STEP_SUMMARY
            echo "| ‚è≠Ô∏è Skipped | ${SKIPPED:-0} |" >> $GITHUB_STEP_SUMMARY
            echo "| **Total** | **${TOTAL:-0}** |" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          else
            echo "### ‚ö†Ô∏è Tests - Not executed or no results" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload Analysis Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: code-quality-reports
          path: |
            compile.log
            target/checkstyle-result.xml
            target/pmd.xml
            target/spotbugsXml.xml
            target/surefire-reports/
            target/site/
          retention-days: 30
          if-no-files-found: ignore

      - name: Comment on PR
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            let comment = '## üîç Code Quality Analysis Results\n\n';
            
            let hasResults = false;
            
            // Compilation Status
            const compileStatus = '${{ steps.compile.outputs.compile_status }}';
            if (compileStatus === '0') {
              comment += '### üî® Compilation\n‚úÖ **SUCCESS**\n\n';
            } else {
              comment += '### üî® Compilation\n‚ùå **FAILED** - See workflow logs for details\n\n';
              hasResults = true;
              
              // Try to extract error summary
              try {
                if (fs.existsSync('compile.log')) {
                  const log = fs.readFileSync('compile.log', 'utf8');
                  const errorLines = log.split('\n').filter(line => line.includes('[ERROR]')).slice(0, 5);
                  if (errorLines.length > 0) {
                    comment += '<details><summary>Compilation Errors (first 5)</summary>\n\n```\n';
                    comment += errorLines.join('\n');
                    comment += '\n```\n</details>\n\n';
                  }
                }
              } catch (e) {
                console.log('Could not parse compilation errors');
              }
            }
            
            // Parse Checkstyle
            try {
              if (fs.existsSync('target/checkstyle-result.xml')) {
                const checkstyle = fs.readFileSync('target/checkstyle-result.xml', 'utf8');
                const errors = (checkstyle.match(/severity="error"/g) || []).length;
                const warnings = (checkstyle.match(/severity="warning"/g) || []).length;
                const info = (checkstyle.match(/severity="info"/g) || []).length;
                
                comment += '### üìã Checkstyle\n';
                comment += `- ‚ùå Errors: ${errors}\n`;
                comment += `- ‚ö†Ô∏è Warnings: ${warnings}\n`;
                comment += `- ‚ÑπÔ∏è Info: ${info}\n\n`;
                hasResults = true;
              }
            } catch (e) {
              comment += '### üìã Checkstyle\n‚ùå No results available\n\n';
            }
            
            // Parse PMD
            try {
              if (fs.existsSync('target/pmd.xml')) {
                const pmd = fs.readFileSync('target/pmd.xml', 'utf8');
                const violations = (pmd.match(/<violation/g) || []).length;
                comment += '### üîç PMD\n';
                comment += `- Violations: ${violations}\n\n`;
                hasResults = true;
              }
            } catch (e) {
              comment += '### üîç PMD\n‚ùå No results available\n\n';
            }
            
            // Parse SpotBugs
            try {
              if (fs.existsSync('target/spotbugsXml.xml')) {
                const spotbugs = fs.readFileSync('target/spotbugsXml.xml', 'utf8');
                const bugs = (spotbugs.match(/<BugInstance/g) || []).length;
                comment += '### üêõ SpotBugs\n';
                comment += `- Potential Bugs: ${bugs}\n\n`;
                hasResults = true;
              }
            } catch (e) {
              comment += '### üêõ SpotBugs\n‚ùå No results available\n\n';
            }
            
            // Test Results
            try {
              const testFiles = fs.readdirSync('target/surefire-reports').filter(f => f.startsWith('TEST-') && f.endsWith('.xml'));
              if (testFiles.length > 0) {
                let totalTests = 0, failures = 0, errors = 0, skipped = 0;
                
                testFiles.forEach(file => {
                  const content = fs.readFileSync(`target/surefire-reports/${file}`, 'utf8');
                  const testsMatch = content.match(/tests="(\d+)"/);
                  const failuresMatch = content.match(/failures="(\d+)"/);
                  const errorsMatch = content.match(/errors="(\d+)"/);
                  const skippedMatch = content.match(/skipped="(\d+)"/);
                  
                  if (testsMatch) totalTests += parseInt(testsMatch[1]);
                  if (failuresMatch) failures += parseInt(failuresMatch[1]);
                  if (errorsMatch) errors += parseInt(errorsMatch[1]);
                  if (skippedMatch) skipped += parseInt(skippedMatch[1]);
                });
                
                const passed = totalTests - failures - errors - skipped;
                comment += '### üß™ Test Results\n';
                comment += `- ‚úÖ Passed: ${passed}\n`;
                comment += `- ‚ùå Failed: ${failures}\n`;
                comment += `- ‚ö†Ô∏è Errors: ${errors}\n`;
                comment += `- ‚è≠Ô∏è Skipped: ${skipped}\n`;
                comment += `- **Total: ${totalTests}**\n\n`;
                hasResults = true;
              }
            } catch (e) {
              comment += '### üß™ Tests\n‚ö†Ô∏è No test results found\n\n';
            }
            
            comment += '---\n';
            comment += `üì¶ [Download detailed reports](${context.payload.repository.html_url}/actions/runs/${context.runId})`;
            
            if (hasResults) {
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            }
